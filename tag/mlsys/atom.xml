<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://example.com</id>
    <title>lcy&#39;s blog • Posts by &#34;mlsys&#34; tag</title>
    <link href="http://example.com" />
    <updated>2023-01-14T12:14:36.000Z</updated>
    <category term="life" />
    <category term="reading record" />
    <category term="paper" />
    <category term="hexo" />
    <category term="kubernetes" />
    <category term="MLsys" />
    <entry>
        <id>http://example.com/2023/01/14/reading-list/</id>
        <title>reading list</title>
        <link rel="alternate" href="http://example.com/2023/01/14/reading-list/"/>
        <content type="html">&lt;p&gt;[摘自网络]&lt;/p&gt;
&lt;span id=&#34;more&#34;&gt;&lt;/span&gt;
&lt;h3 id=&#34;frameworks&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#frameworks&#34;&gt;#&lt;/a&gt; &lt;strong&gt;Frameworks&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[VLDB &#39;20] PyTorch Distributed: Experiences on Accelerating Data Parallel Training&lt;/li&gt;
&lt;li&gt;[NeurIPS &#39;19] PyTorch: An Imperative Style, High-Performance Deep Learning Library&lt;/li&gt;
&lt;li&gt;[OSDI &#39;18] Ray: A Distributed Framework for Emerging AI Applications&lt;/li&gt;
&lt;li&gt;[OSDI &#39;16] TensorFlow: A System for Large-Scale Machine Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parallelism-distributed-systems&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#parallelism-distributed-systems&#34;&gt;#&lt;/a&gt; &lt;strong&gt;Parallelism &amp;amp; Distributed Systems&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[OSDI &#39;22] Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;22] Varuna: Scalable, Low-cost Training of Massive Deep Learning Models&lt;/li&gt;
&lt;li&gt;[SC ‘21’] Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines&lt;/li&gt;
&lt;li&gt;[ICML &#39;21] PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models&lt;/li&gt;
&lt;li&gt;[OSDI &#39;20] A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters&lt;/li&gt;
&lt;li&gt;[ATC &#39;20] HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism&lt;/li&gt;
&lt;li&gt;[NeurIPS &#39;19] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism&lt;/li&gt;
&lt;li&gt;[SOSP &#39;19] A Generic Communication Scheduler for Distributed DNN Training Acceleration&lt;/li&gt;
&lt;li&gt;[SOSP &#39;19] PipeDream: Generalized Pipeline Parallelism for DNN Training&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;19] Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks&lt;/li&gt;
&lt;li&gt;[arXiv &#39;18] Horovod: fast and easy distributed deep learning in TensorFlow&lt;/li&gt;
&lt;li&gt;[ATC &#39;17] Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;16] STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;16] GeePS: Scalable Deep Learning on Distributed GPUs with a GPU-specialized Parameter Server&lt;/li&gt;
&lt;li&gt;[OSDI &#39;14] Scaling Distributed Machine Learning with the Parameter Server&lt;/li&gt;
&lt;li&gt;[NIPS &#39;12] Large Scale Distributed Deep Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gpu-cluster-management&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#gpu-cluster-management&#34;&gt;#&lt;/a&gt; &lt;strong&gt;GPU Cluster Management&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[OSDI &#39;22] Looking Beyond GPUs for DNN Scheduling on Multi-Tenant Clusters&lt;/li&gt;
&lt;li&gt;[NSDI &#39;22] MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters&lt;/li&gt;
&lt;li&gt;[OSDI &#39;21] Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning&lt;/li&gt;
&lt;li&gt;[NSDI &#39;21] Elastic Resource Sharing for Distributed Deep Learning&lt;/li&gt;
&lt;li&gt;[OSDI &#39;20] Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads&lt;/li&gt;
&lt;li&gt;[OSDI &#39;20] AntMan: Dynamic Scaling on GPU Clusters for Deep Learning&lt;/li&gt;
&lt;li&gt;[NSDI &#39;20] Themis: Fair and Efficient GPU Cluster Scheduling&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;20] Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning&lt;/li&gt;
&lt;li&gt;[NSDI &#39;19] Tiresias: A GPU Cluster Manager for Distributed Deep Learning&lt;/li&gt;
&lt;li&gt;[ATC &#39;19] Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads&lt;/li&gt;
&lt;li&gt;[OSDI &#39;18] Gandiva: Introspective cluster scheduling for deep learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;memory-management-for-machine-learning&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#memory-management-for-machine-learning&#34;&gt;#&lt;/a&gt; &lt;strong&gt;Memory Management for Machine Learning&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ATC &#39;22] Memory Harvesting in Multi-GPU Systems with Hierarchical Unified Virtual Memory&lt;/li&gt;
&lt;li&gt;[MobiSys &#39;22] Memory-efficient DNN Training on Mobile Devices&lt;/li&gt;
&lt;li&gt;[HPCA &#39;22] Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems&lt;/li&gt;
&lt;li&gt;[ASPLOS &#39;20] Capuchin: Tensor-based GPU Memory Management for Deep Learning&lt;/li&gt;
&lt;li&gt;[ASPLOS &#39;20] SwapAdvisor: Push Deep Learning Beyond the GPU Memory Limit via Smart Swapping&lt;/li&gt;
&lt;li&gt;[ISCA &#39;19] Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory&lt;/li&gt;
&lt;li&gt;[ISCA &#39;18] Gist: Efficient Data Encoding for Deep Neural Network Training&lt;/li&gt;
&lt;li&gt;[PPoPP &#39;18] SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks&lt;/li&gt;
&lt;li&gt;[MICRO &#39;16] vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scheduling-resource-management&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#scheduling-resource-management&#34;&gt;#&lt;/a&gt; &lt;strong&gt;Scheduling &amp;amp; Resource Management&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[arXiv &#39;22] EasyScale: Accuracy-consistent Elastic Training for Deep Learning&lt;/li&gt;
&lt;li&gt;[MLSys &#39;22] VirtualFlow: Decoupling Deep Learning Models from the Underlying Hardware&lt;/li&gt;
&lt;li&gt;[SIGCOMM &#39;22] Multi-resource interleaving for deep learning training&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;22] Out-Of-Order BackProp: An Effective Scheduling Technique for Deep Learning&lt;/li&gt;
&lt;li&gt;[ATC &#39;21] Zico: Efficient GPU Memory Sharing for Concurrent DNN Training&lt;/li&gt;
&lt;li&gt;[NeurIPS &#39;20] Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning&lt;/li&gt;
&lt;li&gt;[OSDI’ 20] KungFu: Making Training in Distributed Machine Learning Adaptive&lt;/li&gt;
&lt;li&gt;[OSDI &#39;20] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications&lt;/li&gt;
&lt;li&gt;[MLSys &#39;20] Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications&lt;/li&gt;
&lt;li&gt;[SOSP &#39;19] Generic Communication Scheduler for Distributed DNN Training Acceleration&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;18] Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters&lt;/li&gt;
&lt;li&gt;[HPCA &#39;18] Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;serving-systems-inference-acceleration&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#serving-systems-inference-acceleration&#34;&gt;#&lt;/a&gt; &lt;strong&gt;Serving Systems (&amp;amp; inference acceleration)&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[EuroSys &#39;23] Fast and Efficient Model Serving Using Multi-GPUs with Direct-Host-Access&lt;/li&gt;
&lt;li&gt;[MICRO &#39;22] DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation&lt;/li&gt;
&lt;li&gt;[ATC &#39;22] Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing&lt;/li&gt;
&lt;li&gt;[OSDI &#39;22] Orca: A Distributed Serving System for Transformer-Based Language Generation Tasks&lt;/li&gt;
&lt;li&gt;[OSDI &#39;22] Achieving μs-scale Preemption for Concurrent GPU-accelerated DNN Inferences&lt;/li&gt;
&lt;li&gt;[ATC &#39;21] INFaaS: Automated Model-less Inference Serving&lt;/li&gt;
&lt;li&gt;[OSDI &#39;20] Serving DNNs like Clockwork: Performance Predictability from the Bottom Up&lt;/li&gt;
&lt;li&gt;[ISCA &#39;20] MLPerf Inference Benchmark&lt;/li&gt;
&lt;li&gt;[SOSP &#39;19] Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis&lt;/li&gt;
&lt;li&gt;[ISCA &#39;19] MnnFast: a fast and scalable system architecture for memory-augmented neural networks&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;19] μLayer: Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;19] GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks&lt;/li&gt;
&lt;li&gt;[OSDI &#39;18] Pretzel: Opening the Black Box of Machine Learning Prediction Serving Systems&lt;/li&gt;
&lt;li&gt;[NSDI &#39;17] Clipper: A Low-Latency Online Prediction Serving System&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-learning-compiler&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#deep-learning-compiler&#34;&gt;#&lt;/a&gt; &lt;strong&gt;Deep Learning Compiler&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[PLDI &#39;21] DeepCuts: A Deep Learning Optimization Framework for Versatile GPU Workloads&lt;/li&gt;
&lt;li&gt;[OSDI &#39;18] TVM: An Automated End-to-End Optimizing Compiler for Deep Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;very-large-models&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#very-large-models&#34;&gt;#&lt;/a&gt; &lt;strong&gt;Very Large Models&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[arxiv &#39;21] ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning&lt;/li&gt;
&lt;li&gt;[ATC &#39;21] ZeRO-Offload: Democratizing Billion-Scale Model Training&lt;/li&gt;
&lt;li&gt;[FAST &#39;21] Behemoth: A Flash-centric Training Accelerator for Extreme-scale DNNs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-learning-recommendation-models&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#deep-learning-recommendation-models&#34;&gt;#&lt;/a&gt; &lt;strong&gt;Deep Learning Recommendation Models&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[OSDI &#39;22] FAERY: An FPGA-accelerated Embedding-based Retrieval System&lt;/li&gt;
&lt;li&gt;[OSDI &#39;22] Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update&lt;/li&gt;
&lt;li&gt;[EuroSys &#39;22] Fleche: An Efficient GPU Embedding Cache for Personalized Recommendations&lt;/li&gt;
&lt;li&gt;[ASPLOS &#39;22] RecShard: statistical feature-based memory optimization for industry-scale neural recommendation&lt;/li&gt;
&lt;li&gt;[HPCA &#39;22] Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized Recommendation&lt;/li&gt;
&lt;li&gt;[MLSys &#39;21] TT-Rec: Tensor Train Compression for Deep Learning Recommendation Model Embeddings&lt;/li&gt;
&lt;li&gt;[HPCA &#39;21] Tensor Casting: Co-Designing Algorithm-Architecture for Personalized Recommendation Training&lt;/li&gt;
&lt;li&gt;[HPCA &#39;21] Understanding Training Efficiency of Deep Learning Recommendation Models at Scale&lt;/li&gt;
&lt;li&gt;[ISCA &#39;20] DeepRecSys: A System for Optimizing End-To-End At-scale Neural Recommendation Inference&lt;/li&gt;
&lt;li&gt;[HPCA &#39;20] The Architectural Implications of Facebook’s DNN-based Personalized Recommendation&lt;/li&gt;
&lt;li&gt;[MICRO &#39;19] TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hardware-support-for-ml&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#hardware-support-for-ml&#34;&gt;#&lt;/a&gt; &lt;strong&gt;Hardware Support for ML&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ISCA &#39;18] A Configurable Cloud-Scale DNN Processor for Real-Time AI&lt;/li&gt;
&lt;li&gt;[ISCA &#39;17] In-Datacenter Performance Analysis of a Tensor Processing Unit&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ml-at-mobile-embedded-systems&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#ml-at-mobile-embedded-systems&#34;&gt;#&lt;/a&gt; &lt;strong&gt;ML at Mobile &amp;amp; Embedded Systems&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[MobiCom &#39;20] SPINN: Synergistic Progressive Inference of Neural Networks over Device and Cloud&lt;/li&gt;
&lt;li&gt;[RTSS &#39;19] Pipelined Data-Parallel CPU/GPU Scheduling for Multi-DNN Real-Time Inference&lt;/li&gt;
&lt;li&gt;[ASPLOS &#39;17] Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ml-techniques-for-systems&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#ml-techniques-for-systems&#34;&gt;#&lt;/a&gt; &lt;strong&gt;ML Techniques for Systems&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ICML &#39;20] An Imitation Learning Approach for Cache Replacement&lt;/li&gt;
&lt;li&gt;[ICML &#39;18] Learning Memory Access Patterns&lt;/li&gt;
&lt;/ul&gt;
</content>
        <category term="MLsys" />
        <updated>2023-01-14T12:14:36.000Z</updated>
    </entry>
</feed>
