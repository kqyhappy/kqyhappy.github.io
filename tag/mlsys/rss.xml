<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>lcy&#39;s blog • Posts by &#34;mlsys&#34; tag</title>
        <link>http://example.com</link>
        <description>学习/科研/生活/阅读</description>
        <language>zh-CN</language>
        <pubDate>Sat, 14 Jan 2023 20:14:36 +0800</pubDate>
        <lastBuildDate>Sat, 14 Jan 2023 20:14:36 +0800</lastBuildDate>
        <category>life</category>
        <category>reading record</category>
        <category>paper</category>
        <category>hexo</category>
        <category>kubernetes</category>
        <category>MLsys</category>
        <category>trash-bin</category>
        <item>
            <guid isPermalink="true">http://example.com/2023/01/14/reading-list/</guid>
            <title>reading list</title>
            <link>http://example.com/2023/01/14/reading-list/</link>
            <category>MLsys</category>
            <pubDate>Sat, 14 Jan 2023 20:14:36 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;[摘自网络]&lt;/p&gt;
&lt;span id=&#34;more&#34;&gt;&lt;/span&gt;

&lt;h3 id=&#34;Frameworks&#34;&gt;&lt;a href=&#34;#Frameworks&#34; class=&#34;headerlink&#34; title=&#34;Frameworks&#34;&gt;&lt;/a&gt;&lt;strong&gt;Frameworks&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[VLDB ‘20] PyTorch Distributed: Experiences on Accelerating Data Parallel Training&lt;/li&gt;
&lt;li&gt;[NeurIPS ‘19] PyTorch: An Imperative Style, High-Performance Deep Learning Library&lt;/li&gt;
&lt;li&gt;[OSDI ‘18] Ray: A Distributed Framework for Emerging AI Applications&lt;/li&gt;
&lt;li&gt;[OSDI ‘16] TensorFlow: A System for Large-Scale Machine Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Parallelism-amp-Distributed-Systems&#34;&gt;&lt;a href=&#34;#Parallelism-amp-Distributed-Systems&#34; class=&#34;headerlink&#34; title=&#34;Parallelism &amp;amp; Distributed Systems&#34;&gt;&lt;/a&gt;&lt;strong&gt;Parallelism &amp;amp; Distributed Systems&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[OSDI ‘22] Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization&lt;/li&gt;
&lt;li&gt;[EuroSys ‘22] Varuna: Scalable, Low-cost Training of Massive Deep Learning Models&lt;/li&gt;
&lt;li&gt;[SC ‘21’] Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines&lt;/li&gt;
&lt;li&gt;[ICML ‘21] PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models&lt;/li&gt;
&lt;li&gt;[OSDI ‘20] A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU&amp;#x2F;CPU Clusters&lt;/li&gt;
&lt;li&gt;[ATC ‘20] HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism&lt;/li&gt;
&lt;li&gt;[NeurIPS ‘19] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism&lt;/li&gt;
&lt;li&gt;[SOSP ‘19] A Generic Communication Scheduler for Distributed DNN Training Acceleration&lt;/li&gt;
&lt;li&gt;[SOSP ‘19] PipeDream: Generalized Pipeline Parallelism for DNN Training&lt;/li&gt;
&lt;li&gt;[EuroSys ‘19] Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks&lt;/li&gt;
&lt;li&gt;[arXiv ‘18] Horovod: fast and easy distributed deep learning in TensorFlow&lt;/li&gt;
&lt;li&gt;[ATC ‘17] Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters&lt;/li&gt;
&lt;li&gt;[EuroSys ‘16] STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning&lt;/li&gt;
&lt;li&gt;[EuroSys ‘16] GeePS: Scalable Deep Learning on Distributed GPUs with a GPU-specialized Parameter Server&lt;/li&gt;
&lt;li&gt;[OSDI ‘14] Scaling Distributed Machine Learning with the Parameter Server&lt;/li&gt;
&lt;li&gt;[NIPS ‘12] Large Scale Distributed Deep Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;GPU-Cluster-Management&#34;&gt;&lt;a href=&#34;#GPU-Cluster-Management&#34; class=&#34;headerlink&#34; title=&#34;GPU Cluster Management&#34;&gt;&lt;/a&gt;&lt;strong&gt;GPU Cluster Management&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[OSDI ‘22] Looking Beyond GPUs for DNN Scheduling on Multi-Tenant Clusters&lt;/li&gt;
&lt;li&gt;[NSDI ‘22] MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters&lt;/li&gt;
&lt;li&gt;[OSDI ‘21] Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning&lt;/li&gt;
&lt;li&gt;[NSDI ‘21] Elastic Resource Sharing for Distributed Deep Learning&lt;/li&gt;
&lt;li&gt;[OSDI ‘20] Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads&lt;/li&gt;
&lt;li&gt;[OSDI ‘20] AntMan: Dynamic Scaling on GPU Clusters for Deep Learning&lt;/li&gt;
&lt;li&gt;[NSDI ‘20] Themis: Fair and Efficient GPU Cluster Scheduling&lt;/li&gt;
&lt;li&gt;[EuroSys ‘20] Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning&lt;/li&gt;
&lt;li&gt;[NSDI ‘19] Tiresias: A GPU Cluster Manager for Distributed Deep Learning&lt;/li&gt;
&lt;li&gt;[ATC ‘19] Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads&lt;/li&gt;
&lt;li&gt;[OSDI ‘18] Gandiva: Introspective cluster scheduling for deep learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Memory-Management-for-Machine-Learning&#34;&gt;&lt;a href=&#34;#Memory-Management-for-Machine-Learning&#34; class=&#34;headerlink&#34; title=&#34;Memory Management for Machine Learning&#34;&gt;&lt;/a&gt;&lt;strong&gt;Memory Management for Machine Learning&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[ATC ‘22] Memory Harvesting in Multi-GPU Systems with Hierarchical Unified Virtual Memory&lt;/li&gt;
&lt;li&gt;[MobiSys ‘22] Memory-efficient DNN Training on Mobile Devices&lt;/li&gt;
&lt;li&gt;[HPCA ‘22] Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems&lt;/li&gt;
&lt;li&gt;[ASPLOS ‘20] Capuchin: Tensor-based GPU Memory Management for Deep Learning&lt;/li&gt;
&lt;li&gt;[ASPLOS ‘20] SwapAdvisor: Push Deep Learning Beyond the GPU Memory Limit via Smart Swapping&lt;/li&gt;
&lt;li&gt;[ISCA ‘19] Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory&lt;/li&gt;
&lt;li&gt;[ISCA ‘18] Gist: Efficient Data Encoding for Deep Neural Network Training&lt;/li&gt;
&lt;li&gt;[PPoPP ‘18] SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks&lt;/li&gt;
&lt;li&gt;[MICRO ‘16] vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Scheduling-amp-Resource-Management&#34;&gt;&lt;a href=&#34;#Scheduling-amp-Resource-Management&#34; class=&#34;headerlink&#34; title=&#34;Scheduling &amp;amp; Resource Management&#34;&gt;&lt;/a&gt;&lt;strong&gt;Scheduling &amp;amp; Resource Management&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[arXiv ‘22] EasyScale: Accuracy-consistent Elastic Training for Deep Learning&lt;/li&gt;
&lt;li&gt;[MLSys ‘22] VirtualFlow: Decoupling Deep Learning Models from the Underlying Hardware&lt;/li&gt;
&lt;li&gt;[SIGCOMM ‘22] Multi-resource interleaving for deep learning training&lt;/li&gt;
&lt;li&gt;[EuroSys ‘22] Out-Of-Order BackProp: An Effective Scheduling Technique for Deep Learning&lt;/li&gt;
&lt;li&gt;[ATC ‘21] Zico: Efficient GPU Memory Sharing for Concurrent DNN Training&lt;/li&gt;
&lt;li&gt;[NeurIPS ‘20] Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning&lt;/li&gt;
&lt;li&gt;[OSDI’ 20] KungFu: Making Training in Distributed Machine Learning Adaptive&lt;/li&gt;
&lt;li&gt;[OSDI ‘20] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications&lt;/li&gt;
&lt;li&gt;[MLSys ‘20] Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications&lt;/li&gt;
&lt;li&gt;[SOSP ‘19] Generic Communication Scheduler for Distributed DNN Training Acceleration&lt;/li&gt;
&lt;li&gt;[EuroSys ‘18] Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters&lt;/li&gt;
&lt;li&gt;[HPCA ‘18] Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Serving-Systems-amp-inference-acceleration&#34;&gt;&lt;a href=&#34;#Serving-Systems-amp-inference-acceleration&#34; class=&#34;headerlink&#34; title=&#34;Serving Systems (&amp;amp; inference acceleration)&#34;&gt;&lt;/a&gt;&lt;strong&gt;Serving Systems (&amp;amp; inference acceleration)&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[EuroSys ‘23] Fast and Efficient Model Serving Using Multi-GPUs with Direct-Host-Access&lt;/li&gt;
&lt;li&gt;[MICRO ‘22] DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation&lt;/li&gt;
&lt;li&gt;[ATC ‘22] Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing&lt;/li&gt;
&lt;li&gt;[OSDI ‘22] Orca: A Distributed Serving System for Transformer-Based Language Generation Tasks&lt;/li&gt;
&lt;li&gt;[OSDI ‘22] Achieving μs-scale Preemption for Concurrent GPU-accelerated DNN Inferences&lt;/li&gt;
&lt;li&gt;[ATC ‘21] INFaaS: Automated Model-less Inference Serving&lt;/li&gt;
&lt;li&gt;[OSDI ‘20] Serving DNNs like Clockwork: Performance Predictability from the Bottom Up&lt;/li&gt;
&lt;li&gt;[ISCA ‘20] MLPerf Inference Benchmark&lt;/li&gt;
&lt;li&gt;[SOSP ‘19] Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis&lt;/li&gt;
&lt;li&gt;[ISCA ‘19] MnnFast: a fast and scalable system architecture for memory-augmented neural networks&lt;/li&gt;
&lt;li&gt;[EuroSys ‘19] μLayer: Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization&lt;/li&gt;
&lt;li&gt;[EuroSys ‘19] GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks&lt;/li&gt;
&lt;li&gt;[OSDI ‘18] Pretzel: Opening the Black Box of Machine Learning Prediction Serving Systems&lt;/li&gt;
&lt;li&gt;[NSDI ‘17] Clipper: A Low-Latency Online Prediction Serving System&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Deep-Learning-Compiler&#34;&gt;&lt;a href=&#34;#Deep-Learning-Compiler&#34; class=&#34;headerlink&#34; title=&#34;Deep Learning Compiler&#34;&gt;&lt;/a&gt;&lt;strong&gt;Deep Learning Compiler&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[PLDI ‘21] DeepCuts: A Deep Learning Optimization Framework for Versatile GPU Workloads&lt;/li&gt;
&lt;li&gt;[OSDI ‘18] TVM: An Automated End-to-End Optimizing Compiler for Deep Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Very-Large-Models&#34;&gt;&lt;a href=&#34;#Very-Large-Models&#34; class=&#34;headerlink&#34; title=&#34;Very Large Models&#34;&gt;&lt;/a&gt;&lt;strong&gt;Very Large Models&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[arxiv ‘21] ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning&lt;/li&gt;
&lt;li&gt;[ATC ‘21] ZeRO-Offload: Democratizing Billion-Scale Model Training&lt;/li&gt;
&lt;li&gt;[FAST ‘21] Behemoth: A Flash-centric Training Accelerator for Extreme-scale DNNs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Deep-Learning-Recommendation-Models&#34;&gt;&lt;a href=&#34;#Deep-Learning-Recommendation-Models&#34; class=&#34;headerlink&#34; title=&#34;Deep Learning Recommendation Models&#34;&gt;&lt;/a&gt;&lt;strong&gt;Deep Learning Recommendation Models&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[OSDI ‘22] FAERY: An FPGA-accelerated Embedding-based Retrieval System&lt;/li&gt;
&lt;li&gt;[OSDI ‘22] Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update&lt;/li&gt;
&lt;li&gt;[EuroSys ‘22] Fleche: An Efficient GPU Embedding Cache for Personalized Recommendations&lt;/li&gt;
&lt;li&gt;[ASPLOS ‘22] RecShard: statistical feature-based memory optimization for industry-scale neural recommendation&lt;/li&gt;
&lt;li&gt;[HPCA ‘22] Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized Recommendation&lt;/li&gt;
&lt;li&gt;[MLSys ‘21] TT-Rec: Tensor Train Compression for Deep Learning Recommendation Model Embeddings&lt;/li&gt;
&lt;li&gt;[HPCA ‘21] Tensor Casting: Co-Designing Algorithm-Architecture for Personalized Recommendation Training&lt;/li&gt;
&lt;li&gt;[HPCA ‘21] Understanding Training Efficiency of Deep Learning Recommendation Models at Scale&lt;/li&gt;
&lt;li&gt;[ISCA ‘20] DeepRecSys: A System for Optimizing End-To-End At-scale Neural Recommendation Inference&lt;/li&gt;
&lt;li&gt;[HPCA ‘20] The Architectural Implications of Facebook’s DNN-based Personalized Recommendation&lt;/li&gt;
&lt;li&gt;[MICRO ‘19] TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Hardware-Support-for-ML&#34;&gt;&lt;a href=&#34;#Hardware-Support-for-ML&#34; class=&#34;headerlink&#34; title=&#34;Hardware Support for ML&#34;&gt;&lt;/a&gt;&lt;strong&gt;Hardware Support for ML&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[ISCA ‘18] A Configurable Cloud-Scale DNN Processor for Real-Time AI&lt;/li&gt;
&lt;li&gt;[ISCA ‘17] In-Datacenter Performance Analysis of a Tensor Processing Unit&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ML-at-Mobile-amp-Embedded-Systems&#34;&gt;&lt;a href=&#34;#ML-at-Mobile-amp-Embedded-Systems&#34; class=&#34;headerlink&#34; title=&#34;ML at Mobile &amp;amp; Embedded Systems&#34;&gt;&lt;/a&gt;&lt;strong&gt;ML at Mobile &amp;amp; Embedded Systems&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[MobiCom ‘20] SPINN: Synergistic Progressive Inference of Neural Networks over Device and Cloud&lt;/li&gt;
&lt;li&gt;[RTSS ‘19] Pipelined Data-Parallel CPU&amp;#x2F;GPU Scheduling for Multi-DNN Real-Time Inference&lt;/li&gt;
&lt;li&gt;[ASPLOS ‘17] Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ML-Techniques-for-Systems&#34;&gt;&lt;a href=&#34;#ML-Techniques-for-Systems&#34; class=&#34;headerlink&#34; title=&#34;ML Techniques for Systems&#34;&gt;&lt;/a&gt;&lt;strong&gt;ML Techniques for Systems&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[ICML ‘20] An Imitation Learning Approach for Cache Replacement&lt;/li&gt;
&lt;li&gt;[ICML ‘18] Learning Memory Access Patterns&lt;/li&gt;
&lt;/ul&gt;
 ]]></description>
        </item>
    </channel>
</rss>
