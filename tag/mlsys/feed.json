{
    "version": "https://jsonfeed.org/version/1",
    "title": "lcy's blog • All posts by \"mlsys\" tag",
    "description": "学习/科研/生活/阅读",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2023/01/14/reading-list/",
            "url": "http://example.com/2023/01/14/reading-list/",
            "title": "reading list",
            "date_published": "2023-01-14T12:14:36.000Z",
            "content_html": "<p>[摘自网络]</p>\n<span id=\"more\"></span>\n\n<h3 id=\"Frameworks\"><a href=\"#Frameworks\" class=\"headerlink\" title=\"Frameworks\"></a><strong>Frameworks</strong></h3><ul>\n<li>[VLDB ‘20] PyTorch Distributed: Experiences on Accelerating Data Parallel Training</li>\n<li>[NeurIPS ‘19] PyTorch: An Imperative Style, High-Performance Deep Learning Library</li>\n<li>[OSDI ‘18] Ray: A Distributed Framework for Emerging AI Applications</li>\n<li>[OSDI ‘16] TensorFlow: A System for Large-Scale Machine Learning</li>\n</ul>\n<h3 id=\"Parallelism-amp-Distributed-Systems\"><a href=\"#Parallelism-amp-Distributed-Systems\" class=\"headerlink\" title=\"Parallelism &amp; Distributed Systems\"></a><strong>Parallelism &amp; Distributed Systems</strong></h3><ul>\n<li>[OSDI ‘22] Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization</li>\n<li>[EuroSys ‘22] Varuna: Scalable, Low-cost Training of Massive Deep Learning Models</li>\n<li>[SC ‘21’] Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines</li>\n<li>[ICML ‘21] PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models</li>\n<li>[OSDI ‘20] A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU&#x2F;CPU Clusters</li>\n<li>[ATC ‘20] HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism</li>\n<li>[NeurIPS ‘19] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</li>\n<li>[SOSP ‘19] A Generic Communication Scheduler for Distributed DNN Training Acceleration</li>\n<li>[SOSP ‘19] PipeDream: Generalized Pipeline Parallelism for DNN Training</li>\n<li>[EuroSys ‘19] Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks</li>\n<li>[arXiv ‘18] Horovod: fast and easy distributed deep learning in TensorFlow</li>\n<li>[ATC ‘17] Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters</li>\n<li>[EuroSys ‘16] STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning</li>\n<li>[EuroSys ‘16] GeePS: Scalable Deep Learning on Distributed GPUs with a GPU-specialized Parameter Server</li>\n<li>[OSDI ‘14] Scaling Distributed Machine Learning with the Parameter Server</li>\n<li>[NIPS ‘12] Large Scale Distributed Deep Networks</li>\n</ul>\n<h3 id=\"GPU-Cluster-Management\"><a href=\"#GPU-Cluster-Management\" class=\"headerlink\" title=\"GPU Cluster Management\"></a><strong>GPU Cluster Management</strong></h3><ul>\n<li>[OSDI ‘22] Looking Beyond GPUs for DNN Scheduling on Multi-Tenant Clusters</li>\n<li>[NSDI ‘22] MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters</li>\n<li>[OSDI ‘21] Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning</li>\n<li>[NSDI ‘21] Elastic Resource Sharing for Distributed Deep Learning</li>\n<li>[OSDI ‘20] Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads</li>\n<li>[OSDI ‘20] AntMan: Dynamic Scaling on GPU Clusters for Deep Learning</li>\n<li>[NSDI ‘20] Themis: Fair and Efficient GPU Cluster Scheduling</li>\n<li>[EuroSys ‘20] Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning</li>\n<li>[NSDI ‘19] Tiresias: A GPU Cluster Manager for Distributed Deep Learning</li>\n<li>[ATC ‘19] Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads</li>\n<li>[OSDI ‘18] Gandiva: Introspective cluster scheduling for deep learning</li>\n</ul>\n<h3 id=\"Memory-Management-for-Machine-Learning\"><a href=\"#Memory-Management-for-Machine-Learning\" class=\"headerlink\" title=\"Memory Management for Machine Learning\"></a><strong>Memory Management for Machine Learning</strong></h3><ul>\n<li>[ATC ‘22] Memory Harvesting in Multi-GPU Systems with Hierarchical Unified Virtual Memory</li>\n<li>[MobiSys ‘22] Memory-efficient DNN Training on Mobile Devices</li>\n<li>[HPCA ‘22] Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems</li>\n<li>[ASPLOS ‘20] Capuchin: Tensor-based GPU Memory Management for Deep Learning</li>\n<li>[ASPLOS ‘20] SwapAdvisor: Push Deep Learning Beyond the GPU Memory Limit via Smart Swapping</li>\n<li>[ISCA ‘19] Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory</li>\n<li>[ISCA ‘18] Gist: Efficient Data Encoding for Deep Neural Network Training</li>\n<li>[PPoPP ‘18] SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks</li>\n<li>[MICRO ‘16] vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design</li>\n</ul>\n<h3 id=\"Scheduling-amp-Resource-Management\"><a href=\"#Scheduling-amp-Resource-Management\" class=\"headerlink\" title=\"Scheduling &amp; Resource Management\"></a><strong>Scheduling &amp; Resource Management</strong></h3><ul>\n<li>[arXiv ‘22] EasyScale: Accuracy-consistent Elastic Training for Deep Learning</li>\n<li>[MLSys ‘22] VirtualFlow: Decoupling Deep Learning Models from the Underlying Hardware</li>\n<li>[SIGCOMM ‘22] Multi-resource interleaving for deep learning training</li>\n<li>[EuroSys ‘22] Out-Of-Order BackProp: An Effective Scheduling Technique for Deep Learning</li>\n<li>[ATC ‘21] Zico: Efficient GPU Memory Sharing for Concurrent DNN Training</li>\n<li>[NeurIPS ‘20] Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning</li>\n<li>[OSDI’ 20] KungFu: Making Training in Distributed Machine Learning Adaptive</li>\n<li>[OSDI ‘20] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications</li>\n<li>[MLSys ‘20] Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications</li>\n<li>[SOSP ‘19] Generic Communication Scheduler for Distributed DNN Training Acceleration</li>\n<li>[EuroSys ‘18] Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters</li>\n<li>[HPCA ‘18] Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective</li>\n</ul>\n<h3 id=\"Serving-Systems-amp-inference-acceleration\"><a href=\"#Serving-Systems-amp-inference-acceleration\" class=\"headerlink\" title=\"Serving Systems (&amp; inference acceleration)\"></a><strong>Serving Systems (&amp; inference acceleration)</strong></h3><ul>\n<li>[EuroSys ‘23] Fast and Efficient Model Serving Using Multi-GPUs with Direct-Host-Access</li>\n<li>[MICRO ‘22] DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation</li>\n<li>[ATC ‘22] Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing</li>\n<li>[OSDI ‘22] Orca: A Distributed Serving System for Transformer-Based Language Generation Tasks</li>\n<li>[OSDI ‘22] Achieving μs-scale Preemption for Concurrent GPU-accelerated DNN Inferences</li>\n<li>[ATC ‘21] INFaaS: Automated Model-less Inference Serving</li>\n<li>[OSDI ‘20] Serving DNNs like Clockwork: Performance Predictability from the Bottom Up</li>\n<li>[ISCA ‘20] MLPerf Inference Benchmark</li>\n<li>[SOSP ‘19] Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis</li>\n<li>[ISCA ‘19] MnnFast: a fast and scalable system architecture for memory-augmented neural networks</li>\n<li>[EuroSys ‘19] μLayer: Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization</li>\n<li>[EuroSys ‘19] GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks</li>\n<li>[OSDI ‘18] Pretzel: Opening the Black Box of Machine Learning Prediction Serving Systems</li>\n<li>[NSDI ‘17] Clipper: A Low-Latency Online Prediction Serving System</li>\n</ul>\n<h3 id=\"Deep-Learning-Compiler\"><a href=\"#Deep-Learning-Compiler\" class=\"headerlink\" title=\"Deep Learning Compiler\"></a><strong>Deep Learning Compiler</strong></h3><ul>\n<li>[PLDI ‘21] DeepCuts: A Deep Learning Optimization Framework for Versatile GPU Workloads</li>\n<li>[OSDI ‘18] TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</li>\n</ul>\n<h3 id=\"Very-Large-Models\"><a href=\"#Very-Large-Models\" class=\"headerlink\" title=\"Very Large Models\"></a><strong>Very Large Models</strong></h3><ul>\n<li>[arxiv ‘21] ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</li>\n<li>[ATC ‘21] ZeRO-Offload: Democratizing Billion-Scale Model Training</li>\n<li>[FAST ‘21] Behemoth: A Flash-centric Training Accelerator for Extreme-scale DNNs</li>\n</ul>\n<h3 id=\"Deep-Learning-Recommendation-Models\"><a href=\"#Deep-Learning-Recommendation-Models\" class=\"headerlink\" title=\"Deep Learning Recommendation Models\"></a><strong>Deep Learning Recommendation Models</strong></h3><ul>\n<li>[OSDI ‘22] FAERY: An FPGA-accelerated Embedding-based Retrieval System</li>\n<li>[OSDI ‘22] Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update</li>\n<li>[EuroSys ‘22] Fleche: An Efficient GPU Embedding Cache for Personalized Recommendations</li>\n<li>[ASPLOS ‘22] RecShard: statistical feature-based memory optimization for industry-scale neural recommendation</li>\n<li>[HPCA ‘22] Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized Recommendation</li>\n<li>[MLSys ‘21] TT-Rec: Tensor Train Compression for Deep Learning Recommendation Model Embeddings</li>\n<li>[HPCA ‘21] Tensor Casting: Co-Designing Algorithm-Architecture for Personalized Recommendation Training</li>\n<li>[HPCA ‘21] Understanding Training Efficiency of Deep Learning Recommendation Models at Scale</li>\n<li>[ISCA ‘20] DeepRecSys: A System for Optimizing End-To-End At-scale Neural Recommendation Inference</li>\n<li>[HPCA ‘20] The Architectural Implications of Facebook’s DNN-based Personalized Recommendation</li>\n<li>[MICRO ‘19] TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning</li>\n</ul>\n<h3 id=\"Hardware-Support-for-ML\"><a href=\"#Hardware-Support-for-ML\" class=\"headerlink\" title=\"Hardware Support for ML\"></a><strong>Hardware Support for ML</strong></h3><ul>\n<li>[ISCA ‘18] A Configurable Cloud-Scale DNN Processor for Real-Time AI</li>\n<li>[ISCA ‘17] In-Datacenter Performance Analysis of a Tensor Processing Unit</li>\n</ul>\n<h3 id=\"ML-at-Mobile-amp-Embedded-Systems\"><a href=\"#ML-at-Mobile-amp-Embedded-Systems\" class=\"headerlink\" title=\"ML at Mobile &amp; Embedded Systems\"></a><strong>ML at Mobile &amp; Embedded Systems</strong></h3><ul>\n<li>[MobiCom ‘20] SPINN: Synergistic Progressive Inference of Neural Networks over Device and Cloud</li>\n<li>[RTSS ‘19] Pipelined Data-Parallel CPU&#x2F;GPU Scheduling for Multi-DNN Real-Time Inference</li>\n<li>[ASPLOS ‘17] Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge</li>\n</ul>\n<h3 id=\"ML-Techniques-for-Systems\"><a href=\"#ML-Techniques-for-Systems\" class=\"headerlink\" title=\"ML Techniques for Systems\"></a><strong>ML Techniques for Systems</strong></h3><ul>\n<li>[ICML ‘20] An Imitation Learning Approach for Cache Replacement</li>\n<li>[ICML ‘18] Learning Memory Access Patterns</li>\n</ul>\n",
            "tags": [
                "MLsys"
            ]
        }
    ]
}