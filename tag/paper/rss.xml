<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>lcy&#39;s blog • Posts by &#34;paper&#34; tag</title>
        <link>http://example.com</link>
        <description>学习/科研/生活/阅读</description>
        <language>zh-CN</language>
        <pubDate>Sat, 14 Jan 2023 21:16:46 +0800</pubDate>
        <lastBuildDate>Sat, 14 Jan 2023 21:16:46 +0800</lastBuildDate>
        <category>reading record</category>
        <category>life</category>
        <category>paper</category>
        <category>hexo</category>
        <category>kubernetes</category>
        <category>MLsys</category>
        <category>ospp</category>
        <item>
            <guid isPermalink="true">http://example.com/2023/01/14/Paper-ATC-2022-GPULet/</guid>
            <title>Paper-ATC&#39;2022-GPULet</title>
            <link>http://example.com/2023/01/14/Paper-ATC-2022-GPULet/</link>
            <category>paper</category>
            <pubDate>Sat, 14 Jan 2023 21:16:46 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;link: &lt;a href=&#34;https://www.usenix.org/conference/atc22/presentation/choi-seungbeom&#34;&gt;Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing | USENIX&lt;/a&gt;&lt;/p&gt;
&lt;span id=&#34;more&#34;&gt;&lt;/span&gt;
&lt;center&gt;
&lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/system-overview.jpg&#34; style=&#34;padding:5px; display:inline;&#34; width=&#34;90%&#34;&gt;
&lt;/center&gt;
&lt;h3 id=&#34;abstract&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#abstract&#34;&gt;#&lt;/a&gt; Abstract&lt;/h3&gt;
&lt;p&gt;本文以&lt;strong&gt;最大化资源利用率&lt;/strong&gt;和&lt;strong&gt;任务吞吐量&lt;/strong&gt;为目标，为多模型推断的 GPU 集群实现了一个在线推断请求调度框架，在物理 GPU 与在线的请求之间设计了 GPULet Scheduler 工具来将 GPU 的计算资源分割成 GPULet，在分割计算资源时，本文综合考虑了&lt;strong&gt;时间&lt;/strong&gt;、&lt;strong&gt;空间&lt;/strong&gt;上的 GPU 资源，以及每次处理请求的&lt;strong&gt;批次&lt;/strong&gt;大小，并且将搜索成本降低到了实际的在线系统可接受的范围。该系统在处理多 DNN 场景时能够&lt;strong&gt;自发调节使用的 GPU 个数&lt;/strong&gt;以节约资源，另外，该系统额外考虑到了不同的 DNN Model 在 GPU 上的并行推断产生的冲突问题，建立了干扰预测模型。&lt;/p&gt;
&lt;h3 id=&#34;introduction&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#introduction&#34;&gt;#&lt;/a&gt; Introduction&lt;/h3&gt;
&lt;p&gt;CPU 上下文切换的速度是微秒级别，而 GPU 是毫秒的级别。&lt;/p&gt;
&lt;h3 id=&#34;background&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#background&#34;&gt;#&lt;/a&gt; Background&lt;/h3&gt;
&lt;p&gt;在 Background 章节，作者从三个维度阐述了提高 GPU 利用率的方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Batching-Aware ML Inference Serving&lt;/p&gt;
&lt;p&gt;与预先加载好数据的模型训练相比，模型推断不能实现 GPU 资源的高利用率的原因是在在线推断场景中，推断请求是在线产生的。人为地设置 GPU 处理推断任务时的批次大小，可以提高 GPU 内核的利用率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Temporal Scheduling for ML on GPUs&lt;/p&gt;
&lt;p&gt;作者介绍了&lt;strong&gt; Nexus&lt;/strong&gt;，通过将 GPU 资源在时间上分片提供给不同批大小的模型来提高 GPU 的利用效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nexus&lt;/strong&gt; 采用&lt;strong&gt; SBP&lt;/strong&gt; 算法。&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/temporal-sharing.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spatial Sharing on GPU&lt;/p&gt;
&lt;p&gt;作者介绍了&lt;strong&gt; GSLICE&lt;/strong&gt;，它的逻辑是根据推断结果的反馈来调整各个分区的资源占比，确定资源占比后启发式地调整批处理的大小，但 GSLICE 只针对单个 GPU。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文的目标是在&lt;strong&gt;空间&lt;/strong&gt;、&lt;strong&gt;时间&lt;/strong&gt;以及&lt;strong&gt;批次大小&lt;/strong&gt;三维的基础分割 GPU 资源实现&lt;strong&gt;最大化资源利用率&lt;/strong&gt;和&lt;strong&gt;最小化使用 GPU&lt;/strong&gt;。作者在 background 章节另外提出了本文的两个 baseline，分别是时间 baseline 的&lt;strong&gt; SBP&lt;/strong&gt; 算法和空间 baseline 的&lt;strong&gt; Greedy best-fit&lt;/strong&gt; 算法。&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/t-s.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;h3 id=&#34;motivation&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#motivation&#34;&gt;#&lt;/a&gt; Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Pre-Experiment-1：推断任务的最佳&lt;strong&gt;批&lt;/strong&gt;大小与 GPU 的&lt;strong&gt;空间分区&lt;/strong&gt;之间存在紧密的联系。&lt;/p&gt;
&lt;p&gt;作者选取不同的模型&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/fig4.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pre-Experiment-2：&lt;strong&gt;时 / 空 / 批&lt;/strong&gt;三维的搜索空间得到的推断任务调度最优解比&lt;strong&gt;时 / 批&lt;/strong&gt;二维的搜索空间得到的推断任务调度最优解更优。&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/3d-search.jpg&#34; width=&#34;80%&#34;&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/fig5.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pre-Experiment-3：在&lt;strong&gt;有效分割&lt;/strong&gt;的前提下，时空 GPU 资源调度可以有效提高 GPU 的资源利用率，提高系统的吞吐量。&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/fig6.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pre-Experiment-4：&lt;strong&gt;不同&lt;/strong&gt;的 Model 同时在一个 GPU 上进行推断任务时，由于存在&lt;strong&gt;资源争用&lt;/strong&gt;问题，并行执行可能会存在额外的&lt;strong&gt;干扰&lt;/strong&gt;开销。&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/fig7.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;design&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#design&#34;&gt;#&lt;/a&gt; Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GPULet&lt;/p&gt;
&lt;p&gt;为了给不同的模型分配的 GPU 资源，作者引入了&lt;strong&gt; Gpulet&lt;/strong&gt;，Gpulet 是建立在物理 GPU 上的一个虚拟的 GPU，它是 GPU 在&lt;strong&gt;空间&lt;/strong&gt;和&lt;strong&gt;时间&lt;/strong&gt;上分配的计算资源的集成。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For each trained ML model, a minimal performance profile is collected offline.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/gpulet.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Overview&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/fig8.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;p&gt;&lt;strong&gt;Request Monitor：&lt;strong&gt;监视不同 Model 的 Request 频率，作为调度器调度的&lt;/strong&gt;参考变量&lt;/strong&gt;之一。&lt;/p&gt;
&lt;p&gt;**Gpulet Scheduler：** 综合配置文件中不同 Model 的时延、SLO 要求、Request 的到达频率，决定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该 Model 的 Request 以何种&lt;strong&gt;批次大小&lt;/strong&gt;执行。&lt;/li&gt;
&lt;li&gt;该 Model 的 Request 在&lt;strong&gt;哪块 GPU&lt;/strong&gt; 上执行。&lt;/li&gt;
&lt;li&gt;该 Model 的 Request 在 GPU 的&lt;strong&gt;空间分区、时间分区&lt;/strong&gt;的资源占用情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;**Partition Manager：** 实现 GPU 的分区，并且周期性的根据 Request 的 Rate 进行调整。&lt;/p&gt;
&lt;p&gt;**Executor：** 实现 Scheduler 的决策，将 Request 分配到不同的 Gpulet 上执行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Challenge-1: Achieve Cost-effective scheduling&lt;/p&gt;
&lt;p&gt;本文实现的核心调度算法，目标是&lt;strong&gt;最大化系统的吞吐量&lt;/strong&gt;的同时&lt;strong&gt;最小化资源消耗&lt;/strong&gt;。相比二维的情况，三维显著增加了调度的搜索空间，因此作者引入了配置文件，记录了 Model 在不同的 batch size 下不同的&lt;strong&gt; Gpu 空间分区&lt;/strong&gt;的时延大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Challenge-2: Dynamic reorganization&lt;/p&gt;
&lt;p&gt;当 Model 的 Request Rate 发生变化时，触发 GPULet 的&lt;strong&gt;重新调度&lt;/strong&gt;。在 Gpu 上实现新的分区需要一定的代价开销，原文引用如下。解决的方法是周期性地&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Preparing a new partition includes spawning a new process, loading kernels used by PyTorch, loading required models, and warming up.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/fig11.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Challenge-3: Interference prediction&lt;/p&gt;
&lt;p&gt;量化不同的模型在同一个物理的 GPU 上并行处理时的&lt;strong&gt;时延干扰损失&lt;/strong&gt;，作者在衡量多个因素后选取了两个重要因素，通过&lt;strong&gt;线性回归&lt;/strong&gt;判定 Model A 和 Model B 在同一块物理 GPU 上并行执行时的时延的&lt;strong&gt;额外开销&lt;/strong&gt;，实验证明该错误率在 10% 左右。&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/interference.jpg&#34; width=&#34;80%&#34;&gt;
    &lt;img src=&#34;/2023/01/14/Paper-ATC-2022-GPULet/fig10.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#evaluation&#34;&gt;#&lt;/a&gt; Evaluation&lt;/h3&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#conclusion&#34;&gt;#&lt;/a&gt; Conclusion&lt;/h3&gt;
 ]]></description>
        </item>
    </channel>
</rss>
